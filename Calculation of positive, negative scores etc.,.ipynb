{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb0cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff81cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_list=[ url links separated by commas] #list of links to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fef426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetPrepare(urls_list):\n",
    "    news_data = []\n",
    "    for url in urls_list:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}\n",
    "        data = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        headlines = soup.find_all('h1', class_=['entry-title'])\n",
    "        article = soup.find_all('div', class_=['td-post-content'])\n",
    "        news_articles = [{'news_headline': headlines[0].text,\n",
    "                  'news_article': article[0].text}]\n",
    "        news_data.extend(news_articles) \n",
    "    df =  pd.DataFrame(news_data)\n",
    "    df = df[['news_headline', 'news_article']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a2a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = datasetPrepare(urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12795e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 170 entries, 0 to 169\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   news_headline  170 non-null    object\n",
      " 1   news_article   170 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf4b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa2813",
   "metadata": {},
   "source": [
    "# Text Wrangling and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b82fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a08957",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe51d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maheg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcba2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df88261",
   "metadata": {},
   "source": [
    "## Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c691ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some important text'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "strip_html_tags('<html><h2>Some important text</h2></html>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe55b70",
   "metadata": {},
   "source": [
    "## Remove accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c87671b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89f94a",
   "metadata": {},
   "source": [
    "## Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ffa214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88c2740f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae59c7",
   "metadata": {},
   "source": [
    "## Text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "207fbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c39cebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash ! his crashed yesterday , ours crash daily'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a330eb",
   "metadata": {},
   "source": [
    "## Text stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c0301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf41506",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dce5897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d098cf0",
   "metadata": {},
   "source": [
    "## Building a text normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc826367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "046cd2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df[\"URL_ID\"]=[x for x in range(1,len(news_df[\"news_headline\"])+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4dc4da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['news_article', 'URL_ID', 'news_headline'], dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a670fefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL_ID', 'news_headline', 'news_article']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_cols=news_df.columns.tolist()\n",
    "temp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=news_df[temp_cols]\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b44fa7",
   "metadata": {},
   "source": [
    "## Pre-process and normalize news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc848e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maheg\\AppData\\Local\\Temp/ipykernel_5692/57384961.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]\n"
     ]
    }
   ],
   "source": [
    "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed51017",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
    "norm_corpus = list(news_df['clean_text'])\n",
    "news_df.iloc[1][['full_text', 'clean_text']].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7a0442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID                                                           2\n",
       "news_headline    How does AI help to monitor Retail Shelf watches?\n",
       "news_article     \\nWith increasing computing power and more dat...\n",
       "full_text        How does AI help to monitor Retail Shelf watch...\n",
       "clean_text       ai help monitor retail shelf watch increase co...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a2530f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36540811",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1=news_df.copy()\n",
    "news_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd4661af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22bbb08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.221"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positive_score(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)[\"pos\"]\n",
    "positive_score(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acebbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def negative_score(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)[\"neg\"]\n",
    "negative_score(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9472a2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.13297950348970758, subjectivity=0.4593453485290221)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(news_df1[\"clean_text\"][1]).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075ae2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13297950348970758"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment[0]\n",
    "polarity(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97b1d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4593453485290221"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subjectivity(text):\n",
    "    return TextBlob(text).sentiment[1]\n",
    "subjectivity(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1460a09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def break_sentences(text):\n",
    "\tnlp = spacy.load('en_core_web_sm')\n",
    "\tdoc = nlp(text)\n",
    "\treturn list(doc.sents)\n",
    "\n",
    "def word_count(text):\n",
    "\tsentences = break_sentences(text)\n",
    "\twords = 0\n",
    "\tfor sentence in sentences:\n",
    "\t\twords += len([token for token in sentence])\n",
    "\treturn words\n",
    "\n",
    "def sentence_count(text):\n",
    "\tsentences = break_sentences(text)\n",
    "\treturn len(sentences)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "avg_sentence_length((news_df1[\"clean_text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e93441d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def difficult_words(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    # Find all words in the text\n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    "    diff_words_set = set()\n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    "    return len(diff_words_set)\n",
    "difficult_words((news_df1[\"clean_text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "056a1e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.43"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage of complex or difficult words\n",
    "def per_comp_words(text):\n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100)\n",
    "    return round((per_diff_words),2)\n",
    "per_comp_words(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c76b37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.77"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Fog_index(text):\n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100)\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
    "    return round(grade,2)\n",
    "Fog_index((news_df1[\"clean_text\"][1]))  #fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac96ae67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_num_of_words_per_sentence(text):\n",
    "    return word_count(text)/sentence_count(text)\n",
    "avg_num_of_words_per_sentence((news_df1[\"clean_text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5d39af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count((news_df1[\"clean_text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "359d3801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from textstat.textstat import textstatistics\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "def syllable_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return round(ASPW, 1)\n",
    "syllable_per_word((news_df1[\"clean_text\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b6b8556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Count_Personal_Pronouns(text):\n",
    "    count=0\n",
    "    for i in TextBlob(text).pos_tags:\n",
    "        if i[-1]==\"VBP\":\n",
    "            count+=1\n",
    "    return count\n",
    "Count_Personal_Pronouns(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da4b986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.43"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word_length(text):\n",
    "    char_count=0\n",
    "    for i in range(len(TextBlob(text).words)):\n",
    "        char_count+=len(TextBlob(text).words[i])\n",
    "    return round(char_count/(len(TextBlob(text).words)),2)\n",
    "avg_word_length(news_df1[\"clean_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df1[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "625941be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [{'Positive_score': positive_score(news_df1[\"clean_text\"][i]),\n",
    "           'Negative_score': negative_score(news_df1[\"clean_text\"][i]),\n",
    "           \"Polarity_score\": polarity(news_df1[\"clean_text\"][i]),\n",
    "           \"Subjectivity_score\": subjectivity(news_df1[\"clean_text\"][i]),\n",
    "           \"Average_Sentence_Length\": avg_sentence_length(news_df1[\"clean_text\"][i]),\n",
    "           \"Percentage_of_Complex_words\": per_comp_words(news_df1[\"clean_text\"][i]),\n",
    "           \"Fog_Index\": Fog_index(news_df1[\"clean_text\"][i]),\n",
    "           \"Avg_num_of_words_per_Sentence\": avg_num_of_words_per_sentence(news_df1[\"clean_text\"][i]),\n",
    "           \"Complex_word_count\": difficult_words(news_df1[\"clean_text\"][i]),\n",
    "           \"Word_count\": word_count(news_df1[\"clean_text\"][i]),\n",
    "           \"Syllable_per_word\": syllable_per_word(news_df1[\"clean_text\"][i]),\n",
    "           \"Personal_Pronouns\": Count_Personal_Pronouns(news_df1[\"clean_text\"][i]),\n",
    "           \"Avg_word_Length\": avg_word_length(news_df1[\"clean_text\"][i])} for i in range(len(news_df1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3e740be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Average_Sentence_Length</th>\n",
       "      <th>Percentage_of_Complex_words</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Avg_num_of_words_per_Sentence</th>\n",
       "      <th>Complex_word_count</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Syllable_per_word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "      <th>Avg_word_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.076840</td>\n",
       "      <td>0.481237</td>\n",
       "      <td>145.666667</td>\n",
       "      <td>37.99</td>\n",
       "      <td>73.46</td>\n",
       "      <td>145.666667</td>\n",
       "      <td>166</td>\n",
       "      <td>437</td>\n",
       "      <td>1.9</td>\n",
       "      <td>24</td>\n",
       "      <td>6.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.132980</td>\n",
       "      <td>0.459345</td>\n",
       "      <td>195.500000</td>\n",
       "      <td>41.43</td>\n",
       "      <td>94.77</td>\n",
       "      <td>195.500000</td>\n",
       "      <td>162</td>\n",
       "      <td>391</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18</td>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.059490</td>\n",
       "      <td>0.583649</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>35.62</td>\n",
       "      <td>235.45</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>394</td>\n",
       "      <td>1106</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74</td>\n",
       "      <td>6.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.038203</td>\n",
       "      <td>0.475654</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>45.08</td>\n",
       "      <td>123.63</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>119</td>\n",
       "      <td>264</td>\n",
       "      <td>2.1</td>\n",
       "      <td>14</td>\n",
       "      <td>6.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.490402</td>\n",
       "      <td>118.666667</td>\n",
       "      <td>35.11</td>\n",
       "      <td>61.51</td>\n",
       "      <td>118.666667</td>\n",
       "      <td>250</td>\n",
       "      <td>712</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.098422</td>\n",
       "      <td>0.389233</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>30.36</td>\n",
       "      <td>182.75</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>259</td>\n",
       "      <td>853</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.143</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.045331</td>\n",
       "      <td>0.495914</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>30.46</td>\n",
       "      <td>73.58</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>187</td>\n",
       "      <td>614</td>\n",
       "      <td>2.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.042580</td>\n",
       "      <td>0.508566</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>30.73</td>\n",
       "      <td>263.49</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>193</td>\n",
       "      <td>628</td>\n",
       "      <td>2.2</td>\n",
       "      <td>28</td>\n",
       "      <td>6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.492608</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>34.86</td>\n",
       "      <td>144.74</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>114</td>\n",
       "      <td>327</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.174</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.094571</td>\n",
       "      <td>0.390171</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>32.95</td>\n",
       "      <td>101.18</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>145</td>\n",
       "      <td>440</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Positive_score  Negative_score  Polarity_score  Subjectivity_score  \\\n",
       "0             0.098           0.060        0.076840            0.481237   \n",
       "1             0.221           0.046        0.132980            0.459345   \n",
       "2             0.198           0.033        0.059490            0.583649   \n",
       "3             0.161           0.006        0.038203            0.475654   \n",
       "4             0.196           0.039        0.014539            0.490402   \n",
       "..              ...             ...             ...                 ...   \n",
       "165           0.139           0.050        0.098422            0.389233   \n",
       "166           0.143           0.053        0.045331            0.495914   \n",
       "167           0.141           0.154       -0.042580            0.508566   \n",
       "168           0.055           0.087        0.027413            0.492608   \n",
       "169           0.174           0.077        0.094571            0.390171   \n",
       "\n",
       "     Average_Sentence_Length  Percentage_of_Complex_words  Fog_Index  \\\n",
       "0                 145.666667                        37.99      73.46   \n",
       "1                 195.500000                        41.43      94.77   \n",
       "2                 553.000000                        35.62     235.45   \n",
       "3                 264.000000                        45.08     123.63   \n",
       "4                 118.666667                        35.11      61.51   \n",
       "..                       ...                          ...        ...   \n",
       "165               426.500000                        30.36     182.75   \n",
       "166               153.500000                        30.46      73.58   \n",
       "167               628.000000                        30.73     263.49   \n",
       "168               327.000000                        34.86     144.74   \n",
       "169               220.000000                        32.95     101.18   \n",
       "\n",
       "     Avg_num_of_words_per_Sentence  Complex_word_count  Word_count  \\\n",
       "0                       145.666667                 166         437   \n",
       "1                       195.500000                 162         391   \n",
       "2                       553.000000                 394        1106   \n",
       "3                       264.000000                 119         264   \n",
       "4                       118.666667                 250         712   \n",
       "..                             ...                 ...         ...   \n",
       "165                     426.500000                 259         853   \n",
       "166                     153.500000                 187         614   \n",
       "167                     628.000000                 193         628   \n",
       "168                     327.000000                 114         327   \n",
       "169                     220.000000                 145         440   \n",
       "\n",
       "     Syllable_per_word  Personal_Pronouns  Avg_word_Length  \n",
       "0                  1.9                 24             6.11  \n",
       "1                  2.0                 18             6.43  \n",
       "2                  2.0                 74             6.37  \n",
       "3                  2.1                 14             6.66  \n",
       "4                  2.0                 57             6.23  \n",
       "..                 ...                ...              ...  \n",
       "165                2.0                 41             6.51  \n",
       "166                2.1                 17             6.63  \n",
       "167                2.2                 28             6.67  \n",
       "168                2.0                 16             6.49  \n",
       "169                2.0                 31             6.29  \n",
       "\n",
       "[170 rows x 13 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output=pd.DataFrame(scores)\n",
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Output=pd.concat([news_df,Output],axis=1)\n",
    "Final_Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae5cc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Output.to_csv('Final_Output_BC.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c12be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
